{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytest",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQX/NnJ2Cgb841tp4WAsFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KMJ-KimMinJu/oss_github/blob/main/pytest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACDwEMF2rfmI",
        "outputId": "3da4c813-632c-4271-8c93-09b4f66d2194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (21.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.12.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnjkVBperkeP",
        "outputId": "83f5dda2-5d0f-4c00-fff8-540ee27495db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: pytest [options] [file_or_dir] [file_or_dir] [...]\n",
            "\n",
            "positional arguments:\n",
            "  file_or_dir\n",
            "\n",
            "general:\n",
            "  -k EXPRESSION         only run tests which match the given substring\n",
            "                        expression. An expression is a python evaluatable\n",
            "                        expression where all names are substring-matched\n",
            "                        against test names and their parent classes. Example:\n",
            "                        -k 'test_method or test_other' matches all test\n",
            "                        functions and classes whose name contains\n",
            "                        'test_method' or 'test_other', while -k 'not\n",
            "                        test_method' matches those that don't contain\n",
            "                        'test_method' in their names. Additionally keywords\n",
            "                        are matched to classes and functions containing extra\n",
            "                        names in their 'extra_keyword_matches' set, as well as\n",
            "                        functions which have names assigned directly to them.\n",
            "  -m MARKEXPR           only run tests matching given mark expression.\n",
            "                        example: -m 'mark1 and not mark2'.\n",
            "  --markers             show markers (builtin, plugin and per-project ones).\n",
            "  -x, --exitfirst       exit instantly on first error or failed test.\n",
            "  --maxfail=num         exit after first num failures or errors.\n",
            "  --strict              marks not registered in configuration file raise\n",
            "                        errors.\n",
            "  -c file               load configuration from `file` instead of trying to\n",
            "                        locate one of the implicit configuration files.\n",
            "  --continue-on-collection-errors\n",
            "                        Force test execution even if collection errors occur.\n",
            "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
            "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
            "                        absolute path: '/home/user/root_dir'; path with\n",
            "                        variables: '$HOME/root_dir'.\n",
            "  --fixtures, --funcargs\n",
            "                        show available fixtures, sorted by plugin appearance\n",
            "                        (fixtures with leading '_' are only shown with '-v')\n",
            "  --fixtures-per-test   show fixtures per test\n",
            "  --import-mode={prepend,append}\n",
            "                        prepend/append to sys.path when importing test\n",
            "                        modules, default is to prepend.\n",
            "  --pdb                 start the interactive Python debugger on errors or\n",
            "                        KeyboardInterrupt.\n",
            "  --pdbcls=modulename:classname\n",
            "                        start a custom interactive Python debugger on errors.\n",
            "                        For example:\n",
            "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
            "  --capture=method      per-test capturing method: one of fd|sys|no.\n",
            "  -s                    shortcut for --capture=no.\n",
            "  --runxfail            run tests even if they are marked xfail\n",
            "  --lf, --last-failed   rerun only the tests that failed at the last run (or\n",
            "                        all if none failed)\n",
            "  --ff, --failed-first  run all tests but run the last failures first. This\n",
            "                        may re-order tests and thus lead to repeated fixture\n",
            "                        setup/teardown\n",
            "  --nf, --new-first     run tests from new files first, then the rest of the\n",
            "                        tests sorted by file mtime\n",
            "  --cache-show          show cache contents, don't perform collection or tests\n",
            "  --cache-clear         remove all cache contents at start of test run.\n",
            "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
            "                        change the behavior when no test failed in the last\n",
            "                        run or no information about the last failures was\n",
            "                        found in the cache\n",
            "\n",
            "reporting:\n",
            "  -v, --verbose         increase verbosity.\n",
            "  -q, --quiet           decrease verbosity.\n",
            "  --verbosity=VERBOSE   set verbosity\n",
            "  -r chars              show extra test summary info as specified by chars\n",
            "                        (f)ailed, (E)error, (s)skipped, (x)failed, (X)passed,\n",
            "                        (p)passed, (P)passed with output, (a)all except pP.\n",
            "                        Warnings are displayed at all times except when\n",
            "                        --disable-warnings is set\n",
            "  --disable-warnings, --disable-pytest-warnings\n",
            "                        disable warnings summary\n",
            "  -l, --showlocals      show locals in tracebacks (disabled by default).\n",
            "  --tb=style            traceback print mode (auto/long/short/line/native/no).\n",
            "  --show-capture={no,stdout,stderr,log,all}\n",
            "                        Controls how captured stdout/stderr/log is shown on\n",
            "                        failed tests. Default is 'all'.\n",
            "  --full-trace          don't cut any tracebacks (default is to cut).\n",
            "  --color=color         color terminal output (yes/no/auto).\n",
            "  --durations=N         show N slowest setup/test durations (N=0 for all).\n",
            "  --pastebin=mode       send failed|all info to bpaste.net pastebin service.\n",
            "  --junit-xml=path      create junit-xml style report file at given path.\n",
            "  --junit-prefix=str    prepend prefix to classnames in junit-xml output\n",
            "  --result-log=path     DEPRECATED path for machine-readable result log.\n",
            "\n",
            "collection:\n",
            "  --collect-only        only collect tests, don't execute them.\n",
            "  --pyargs              try to interpret all arguments as python packages.\n",
            "  --ignore=path         ignore path during collection (multi-allowed).\n",
            "  --deselect=nodeid_prefix\n",
            "                        deselect item during collection (multi-allowed).\n",
            "  --confcutdir=dir      only load conftest.py's relative to specified dir.\n",
            "  --noconftest          Don't load any conftest.py files.\n",
            "  --keep-duplicates     Keep duplicate tests.\n",
            "  --collect-in-virtualenv\n",
            "                        Don't ignore tests in a local virtualenv directory\n",
            "  --doctest-modules     run doctests in all .py modules\n",
            "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
            "                        choose another output format for diffs on doctest\n",
            "                        failure\n",
            "  --doctest-glob=pat    doctests file matching pattern, default: test*.txt\n",
            "  --doctest-ignore-import-errors\n",
            "                        ignore doctest ImportErrors\n",
            "  --doctest-continue-on-failure\n",
            "                        for a given doctest, continue to run after the first\n",
            "                        failure\n",
            "\n",
            "test session debugging and configuration:\n",
            "  --basetemp=dir        base temporary directory for this test run.\n",
            "  --version             display pytest lib version and import information.\n",
            "  -h, --help            show help message and configuration info\n",
            "  -p name               early-load given plugin (multi-allowed). To avoid\n",
            "                        loading of plugins, use the `no:` prefix, e.g.\n",
            "                        `no:doctest`.\n",
            "  --trace-config        trace considerations of conftest.py files.\n",
            "  --debug               store internal tracing debug information in\n",
            "                        'pytestdebug.log'.\n",
            "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
            "                        override ini option with \"option=value\" style, e.g.\n",
            "                        `-o xfail_strict=True -o cache_dir=cache`.\n",
            "  --assert=MODE         Control assertion debugging tools. 'plain' performs no\n",
            "                        assertion debugging. 'rewrite' (the default) rewrites\n",
            "                        assert statements in test modules on import to provide\n",
            "                        assert expression information.\n",
            "  --setup-only          only setup fixtures, do not execute tests.\n",
            "  --setup-show          show setup of fixtures while executing tests.\n",
            "  --setup-plan          show what fixtures and tests would be executed but\n",
            "                        don't execute anything.\n",
            "\n",
            "pytest-warnings:\n",
            "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
            "                        set which warnings to report, see -W option of python\n",
            "                        itself.\n",
            "\n",
            "logging:\n",
            "  --no-print-logs       disable printing caught logs on failed tests.\n",
            "  --log-level=LOG_LEVEL\n",
            "                        logging level used by the logging module\n",
            "  --log-format=LOG_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-date-format=LOG_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "  --log-cli-level=LOG_CLI_LEVEL\n",
            "                        cli logging level.\n",
            "  --log-cli-format=LOG_CLI_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "  --log-file=LOG_FILE   path to a file when logging will be written to.\n",
            "  --log-file-level=LOG_FILE_LEVEL\n",
            "                        log file logging level.\n",
            "  --log-file-format=LOG_FILE_FORMAT\n",
            "                        log format as used by the logging module.\n",
            "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
            "                        log date format as used by the logging module.\n",
            "\n",
            "typeguard:\n",
            "  --typeguard-packages=TYPEGUARD_PACKAGES\n",
            "                        comma separated name list of packages and modules to\n",
            "                        instrument for type checking\n",
            "\n",
            "\n",
            "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg file found:\n",
            "\n",
            "  markers (linelist)       markers for test functions\n",
            "  empty_parameter_set_mark (string) default marker for empty parametersets\n",
            "  norecursedirs (args)     directory patterns to avoid for recursion\n",
            "  testpaths (args)         directories to search for tests when no files or dire\n",
            "  console_output_style (string) console output: classic or with additional progr\n",
            "  usefixtures (args)       list of default fixtures to be used with this project\n",
            "  python_files (args)      glob-style file patterns for Python test module disco\n",
            "  python_classes (args)    prefixes or glob names for Python test class discover\n",
            "  python_functions (args)  prefixes or glob names for Python test function and m\n",
            "  xfail_strict (bool)      default for the strict parameter of xfail markers whe\n",
            "  junit_suite_name (string) Test suite name for JUnit report\n",
            "  junit_logging (string)   Write captured log messages to JUnit report: one of n\n",
            "  doctest_optionflags (args) option flags for doctests\n",
            "  doctest_encoding (string) encoding used for doctest files\n",
            "  cache_dir (string)       cache directory path.\n",
            "  filterwarnings (linelist) Each line specifies a pattern for warnings.filterwar\n",
            "  log_print (bool)         default value for --no-print-logs\n",
            "  log_level (string)       default value for --log-level\n",
            "  log_format (string)      default value for --log-format\n",
            "  log_date_format (string) default value for --log-date-format\n",
            "  log_cli (bool)           enable log display during test run (also known as \"li\n",
            "  log_cli_level (string)   default value for --log-cli-level\n",
            "  log_cli_format (string)  default value for --log-cli-format\n",
            "  log_cli_date_format (string) default value for --log-cli-date-format\n",
            "  log_file (string)        default value for --log-file\n",
            "  log_file_level (string)  default value for --log-file-level\n",
            "  log_file_format (string) default value for --log-file-format\n",
            "  log_file_date_format (string) default value for --log-file-date-format\n",
            "  addopts (args)           extra command line options\n",
            "  minversion (string)      minimally required pytest version\n",
            "\n",
            "environment variables:\n",
            "  PYTEST_ADDOPTS           extra command line options\n",
            "  PYTEST_PLUGINS           comma-separated plugins to load during startup\n",
            "  PYTEST_DEBUG             set to enable debug tracing of pytest's internals\n",
            "\n",
            "\n",
            "to see available markers type: pytest --markers\n",
            "to see available fixtures type: pytest --fixtures\n",
            "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWwpFWrFrpiu",
        "outputId": "783d3430-cba2-4d98-a9c0-7d0faaca2079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_sample.py F.\u001b[36m                                                        [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m      x=5\u001b[0m\n",
            "\u001b[1m      y=6\u001b[0m\n",
            "\u001b[1m      assert x+1 == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m>     assert x == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: test failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 6\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 1 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7AYGiyHsc2l",
        "outputId": "720191e3-6bae-4484-8150-51304364b8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollected 2 items                                                              \u001b[0m\n",
            "\n",
            "test_sample.py::test_file1_method1 \u001b[31mFAILED\u001b[0m\u001b[36m                                [ 50%]\u001b[0m\n",
            "test_sample.py::test_file1_method2 \u001b[32mPASSED\u001b[0m\u001b[36m                                [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m      x=5\u001b[0m\n",
            "\u001b[1m      y=6\u001b[0m\n",
            "\u001b[1m      assert x+1 == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m>     assert x == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: test failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 6\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 1 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhWUInvPtCl0",
        "outputId": "6b349623-f8af-4e52-8330-ed0124b32b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_sample.py F.F\u001b[36m                                                       [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m      x=5\u001b[0m\n",
            "\u001b[1m      y=6\u001b[0m\n",
            "\u001b[1m      assert x+1 == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m>     assert x == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: test failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 6\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method3 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method3():\u001b[0m\n",
            "\u001b[1m>     assert \"hello\" == \"Hai\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'hello' == 'Hai'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       - hello\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + Hai\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:14: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PYDobzRtyYZ",
        "outputId": "54a1d6be-db21-49bf-eaa3-2d9639f4ffca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_sample.py::test_file1_method1 \u001b[31mFAILED\u001b[0m\u001b[36m                                [ 33%]\u001b[0m\n",
            "test_sample.py::test_file1_method2 \u001b[32mPASSED\u001b[0m\u001b[36m                                [ 66%]\u001b[0m\n",
            "test_sample.py::test_file1_method3 \u001b[31mFAILED\u001b[0m\u001b[36m                                [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m      x=5\u001b[0m\n",
            "\u001b[1m      y=6\u001b[0m\n",
            "\u001b[1m      assert x+1 == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m>     assert x == y, \"test failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: test failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 5 == 6\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:6: AssertionError\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method3 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    def test_file1_method3():\u001b[0m\n",
            "\u001b[1m>     assert \"hello\" == \"Hai\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: assert 'hello' == 'Hai'\u001b[0m\n",
            "\u001b[1m\u001b[31mE       - hello\u001b[0m\n",
            "\u001b[1m\u001b[31mE       + Hai\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample.py\u001b[0m:14: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.03 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -k method1 -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l-nxMAZt7rB",
        "outputId": "2a51b2e6-09f8-48af-cbd2-f9eb54ec04f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items / 2 deselected                                               \u001b[0m\n",
            "\n",
            "test_sample1.py::test_file1_method1 \u001b[31mFAILED\u001b[0m\u001b[36m                               [ 50%]\u001b[0m\n",
            "test_sample2.py::test_file1_method1 \u001b[31mFAILED\u001b[0m\u001b[36m                               [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    @pytest.mark.set1\u001b[0m\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m>     assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample1.py\u001b[0m:4: AssertionError\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method1 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    @pytest.mark.set1\u001b[0m\n",
            "\u001b[1m    def test_file1_method1():\u001b[0m\n",
            "\u001b[1m>     assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample2.py\u001b[0m:4: AssertionError\n",
            "\u001b[31m\u001b[1m==================== 2 failed, 2 deselected in 0.05 seconds ====================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -m set2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OnyWML_w5Ec",
        "outputId": "f4a4fc0a-456f-410b-f204-41761de18686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items / 3 deselected                                               \u001b[0m\n",
            "\n",
            "test_sample1.py F\u001b[36m                                                        [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_file1_method2 ______________________________\u001b[0m\n",
            "\n",
            "\u001b[1m    @pytest.mark.set2\u001b[0m\n",
            "\u001b[1m    def test_file1_method2():\u001b[0m\n",
            "\u001b[1m>     assert False\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert False\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_sample1.py\u001b[0m:8: AssertionError\n",
            "\u001b[31m\u001b[1m==================== 1 failed, 3 deselected in 0.03 seconds ====================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test test_fixture.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45qi_AH_yUjy",
        "outputId": "3776c58e-2883-46a0-be7a-4e6f7edb543d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_fixture.py F.F\u001b[36m                                                      [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_comparewithAA ______________________________\u001b[0m\n",
            "\n",
            "supply_AA_BB_CC = [25, 35, 45]\n",
            "\n",
            "\u001b[1m    def test_comparewithAA(supply_AA_BB_CC):\u001b[0m\n",
            "\u001b[1m      zz=35\u001b[0m\n",
            "\u001b[1m>     assert supply_AA_BB_CC[0]==zz, \"aa and zz comparison failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: aa and zz comparison failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 25 == 35\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_fixture.py\u001b[0m:12: AssertionError\n",
            "\u001b[31m\u001b[1m______________________________ test_comparewithCC ______________________________\u001b[0m\n",
            "\n",
            "supply_AA_BB_CC = [25, 35, 45]\n",
            "\n",
            "\u001b[1m    def test_comparewithCC(supply_AA_BB_CC):\u001b[0m\n",
            "\u001b[1m      zz=35\u001b[0m\n",
            "\u001b[1m>     assert supply_AA_BB_CC[2]==zz, \"cc and zz comparison failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: cc and zz comparison failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 45 == 35\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_fixture.py\u001b[0m:20: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test test_basic_fixture.py test_basic_fixture2.py -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkyhIQP40KUP",
        "outputId": "442e3a4f-f81a-4088-944a-46b911c726ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 2 items                                                             \u001b[0m\u001b[1m\rcollecting 4 items                                                             \u001b[0m\u001b[1m\rcollected 4 items                                                              \u001b[0m\n",
            "\n",
            "test_basic_fixture.py::test_comparewithBB \u001b[31mERROR\u001b[0m\u001b[36m                          [ 25%]\u001b[0m\n",
            "test_basic_fixture.py::test_comparewithCC \u001b[31mERROR\u001b[0m\u001b[36m                          [ 50%]\u001b[0m\n",
            "test_basic_fixture2.py::test_comparewithBB \u001b[31mERROR\u001b[0m\u001b[36m                         [ 75%]\u001b[0m\n",
            "test_basic_fixture2.py::test_comparewithCC \u001b[31mERROR\u001b[0m\u001b[36m                         [100%]\u001b[0m\n",
            "\n",
            "==================================== ERRORS ====================================\n",
            "_____________________ ERROR at setup of test_comparewithBB _____________________\n",
            "file /content/test_basic_fixture.py, line 8\n",
            "  def test_comparewithBB(supply_AA_BB_CC):\n",
            "\u001b[31mE       fixture 'supply_AA_BB_CC' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_xml_attribute, record_xml_property, recwarn, test_comparewithAA, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_basic_fixture.py:8\n",
            "_____________________ ERROR at setup of test_comparewithCC _____________________\n",
            "file /content/test_basic_fixture.py, line 12\n",
            "  def test_comparewithCC(supply_AA_BB_CC):\n",
            "\u001b[31mE       fixture 'supply_AA_BB_CC' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_xml_attribute, record_xml_property, recwarn, test_comparewithAA, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_basic_fixture.py:12\n",
            "_____________________ ERROR at setup of test_comparewithBB _____________________\n",
            "file /content/test_basic_fixture2.py, line 8\n",
            "  def test_comparewithBB(supply_AA_BB_CC):\n",
            "\u001b[31mE       fixture 'supply_AA_BB_CC' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_xml_attribute, record_xml_property, recwarn, test_comparewithAA, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_basic_fixture2.py:8\n",
            "_____________________ ERROR at setup of test_comparewithCC _____________________\n",
            "file /content/test_basic_fixture2.py, line 12\n",
            "  def test_comparewithCC(supply_AA_BB_CC):\n",
            "\u001b[31mE       fixture 'supply_AA_BB_CC' not found\u001b[0m\n",
            "\u001b[31m>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, doctest_namespace, monkeypatch, pytestconfig, record_property, record_xml_attribute, record_xml_property, recwarn, test_comparewithAA, tmpdir, tmpdir_factory\u001b[0m\n",
            "\u001b[31m>       use 'pytest --fixtures [testpath]' for help on them.\u001b[0m\n",
            "\n",
            "/content/test_basic_fixture2.py:12\n",
            "\u001b[31m\u001b[1m=========================== 4 error in 0.04 seconds ============================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -v"
      ],
      "metadata": {
        "id": "sYW93Hl82cWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c33888-b0e7-4516-f8ba-c783ca7e1f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 3 items                                                             \u001b[0m\u001b[1m\rcollected 3 items                                                              \u001b[0m\n",
            "\n",
            "test_basic_fixture.py::test_comparewithA \u001b[31mFAILED\u001b[0m\u001b[36m                          [ 33%]\u001b[0m\n",
            "test_basic_fixture.py::test_comparewithB \u001b[32mPASSED\u001b[0m\u001b[36m                          [ 66%]\u001b[0m\n",
            "test_basic_fixture.py::test_comparewithC \u001b[31mFAILED\u001b[0m\u001b[36m                          [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________________ test_comparewithA _______________________________\u001b[0m\n",
            "\n",
            "supply_A_B_C = [25, 35, 45]\n",
            "\n",
            "\u001b[1m    def test_comparewithA(supply_A_B_C):\u001b[0m\n",
            "\u001b[1m      z = 35\u001b[0m\n",
            "\u001b[1m>     assert supply_A_B_C[0] == z, \"a and z comparison failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: a and z comparison failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 25 == 35\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_basic_fixture.py\u001b[0m:5: AssertionError\n",
            "\u001b[31m\u001b[1m______________________________ test_comparewithC _______________________________\u001b[0m\n",
            "\n",
            "supply_A_B_C = [25, 35, 45]\n",
            "\n",
            "\u001b[1m    def test_comparewithC(supply_A_B_C):\u001b[0m\n",
            "\u001b[1m      z = 35\u001b[0m\n",
            "\u001b[1m>     assert supply_A_B_C[2] == z, \"c and z comparison failed\"\u001b[0m\n",
            "\u001b[1m\u001b[31mE     AssertionError: c and z comparison failed\u001b[0m\n",
            "\u001b[1m\u001b[31mE     assert 45 == 35\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtest_basic_fixture.py\u001b[0m:13: AssertionError\n",
            "\u001b[31m\u001b[1m====================== 2 failed, 1 passed in 0.04 seconds ======================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!py.test -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzFBYg2Se33J",
        "outputId": "4c013c6e-40c7-4858-d0a6-26b213a5a28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.7.13, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollecting 0 items                                                             \u001b[0m\u001b[1m\rcollected 0 items                                                              \u001b[0m\n",
            "\n",
            "\u001b[33m\u001b[1m========================= no tests ran in 0.02 seconds =========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RQ3C6Nn0hf1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}